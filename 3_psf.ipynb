{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point spread function using Tiny Tim/Galfit\n",
    "\n",
    "Calling tinytim to generate psf is realized by .py script modified from Bryan R. Gillis, the following infomation is required:\n",
    "\n",
    "0. sensor name\n",
    "\n",
    "1. detector position\n",
    "\n",
    "2. wavelengths of the emission lines (redshift)\n",
    "\n",
    "3. subsampled factor\n",
    "\n",
    "4. (optional) filter used and focus info\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import modules and defing a few useful little functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psf.make_psf as tinytim\n",
    "import  numpy               as     np\n",
    "from    astropy.table       import Table\n",
    "from    astropy.io          import fits\n",
    "from    scipy.ndimage       import rotate\n",
    "#from    astropy.cosmology   import Planck18\n",
    "#import  astropy.units       as     u\n",
    "import  matplotlib.pyplot   as     plt\n",
    "import  matplotlib.colors   as     colors  \n",
    "from    matplotlib          import use\n",
    "from    tqdm                import tqdm\n",
    "from concurrent.futures     import ThreadPoolExecutor, as_completed\n",
    "import sys, os\n",
    "from IPython.display import clear_output\n",
    "from scripts.tools   import *\n",
    "\n",
    "obj_lis = Table.read('obj_lis_selected.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  gen imput parameters for tinytim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 158/158 [01:22<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of obj processed: 158\n",
      "number of failed obj 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def gen_psf_input_param(obj):\n",
    "    #try:\n",
    "    #colnames for param table\n",
    "    col_names = ['identifier', 'rootname',  'pad', 'filter', 'pa' ,'focus', 'coord_ha', 'coord_hb', 'wavelen_ha', 'wavelen_hb','DELTATIM']\n",
    "    beam_path = f\"data_products/{file_name(obj,'beams')}\"\n",
    "    with fits.open(beam_path) as hdu:\n",
    "        rows = []\n",
    "        for image in hdu:\n",
    "            if image.name == 'SCI':\n",
    "                #rootname, pad, shape filter\n",
    "                rootname = image.header['ROOTNAME']\n",
    "                identifier = f\"{obj['subfield']}_{obj['id']}_{image.header['ROOTNAME']}\"\n",
    "                pad        = image.header.get('PAD',0)\n",
    "                shape      = np.array(image.data.shape)\n",
    "                filter     = image.header['filter']\n",
    "\n",
    "                #thumbnail rel. pos\n",
    "                x0_crop = image.header['ORIGINX']\n",
    "                y0_crop = image.header['ORIGINY']\n",
    "\n",
    "                # this part need actual pixel coord for each Ha and Hb wavelen\n",
    "                coord_ha =  np.array((x0_crop,y0_crop)) - pad  #shape/2 +\n",
    "                coord_hb =  np.array((x0_crop,y0_crop)) - pad  #shape/2 \n",
    "\n",
    "                # this still needs focus information\n",
    "                focus = 0\n",
    "                #positon angle\n",
    "                pa = image.header['ORIENTAT']\n",
    "\n",
    "                #generate spectrum for halpha and hb\n",
    "                wavelen_ha = 656.28*(1+obj['z_MAP'])\n",
    "                wavelen_hb = 486.13*(1+obj['z_MAP'])\n",
    "\n",
    "                #integrationtime\n",
    "                deltatim = image.header['DELTATIM']\n",
    "\n",
    "                rows.append((identifier, rootname,  pad, filter, pa ,focus, coord_ha, coord_hb, wavelen_ha, wavelen_hb,deltatim))\n",
    "        \n",
    "        #save tiny tim & drizzle input param table\n",
    "        Table(rows=rows,names=col_names).write(f\"psf/obj_param/{obj['subfield']}_{obj['id']}_psf.fits\",overwrite=True)\n",
    "        return f\"{obj['subfield']}_{obj['id']} processed\"\n",
    "        \n",
    "    #except Exception as e:\n",
    "    #    return f\"!error {obj['subfield']}_{obj['id']}: {e}\"\n",
    "\n",
    "\n",
    "from concurrent.futures     import ThreadPoolExecutor, as_completed\n",
    "def cat_process_gen_psf_input_param(obj_lis,max_threads=1):\n",
    "    #make directories:\n",
    "    os.makedirs('psf/obj_param/',exist_ok=True)\n",
    "    results = []\n",
    "    if max_threads>1:\n",
    "        with ThreadPoolExecutor(max_threads) as executor:                                                       \n",
    "            futures = {executor.submit(gen_psf_input_param,obj):obj for obj in obj_lis}\n",
    "            for future in tqdm(as_completed(futures), total=len(obj_lis), desc=\"Processing\"):\n",
    "                results.append(future.result())\n",
    "        return results\n",
    "    else:\n",
    "        for obj in tqdm(obj_lis):\n",
    "            results.append(gen_psf_input_param(obj))\n",
    "        return results\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    obj_lis = Table.read('obj_lis_selected.fits')\n",
    "    results = cat_process_gen_psf_input_param(obj_lis,max_threads=8)\n",
    "    number = 0\n",
    "    for result in results:\n",
    "        if 'error' in result:\n",
    "            number +=1\n",
    "            print(result)\n",
    "    print('total number of obj processed:',len(results))\n",
    "    print('number of failed obj',number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate psf by calling tinytim + rotation and crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 158/158 [36:42<00:00, 13.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of obj processed: 158\n",
      "number of failed obj 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def rotate_and_crop(image_path,rotating_angle=0,crop_size=50):\n",
    "    with fits.open(image_path) as hdu:\n",
    "\n",
    "        rot = rotate(hdu[0].data, \n",
    "                    angle=rotating_angle, \n",
    "                    reshape=False, \n",
    "                    mode='constant')\n",
    "        \n",
    "        center_x, center_y = np.array(hdu[0].data.shape) // 2\n",
    "        half_crop = crop_size // 2\n",
    "        cropped_data = rot[\n",
    "            center_y - half_crop : center_y + half_crop,\n",
    "            center_x - half_crop : center_x + half_crop]\n",
    "        image = fits.ImageHDU(data=cropped_data, header=hdu[0].header)\n",
    "    return image\n",
    "\n",
    "#using tinytim and rotate to generate single psf\n",
    "def gen_psf(obj,exist_skip=False):\n",
    "    #try:\n",
    "\n",
    "    #path to save individual psf and combined psf\n",
    "    save_path          = f\"psf/individual_psf/{obj['subfield']}_{obj['id']}\"\n",
    "    save_path_combined = f\"psf/combined_psf/{obj['subfield']}_{obj['id']}\"\n",
    "\n",
    "    if os.path.exists(save_path) and exist_skip==True: \n",
    "        clear_output(wait=True)\n",
    "        return f\"{obj['subfield']}_{obj['id']} skipped\"\n",
    "\n",
    "    #load parameter table\n",
    "    param_table = Table.read(f\"psf/obj_param/{obj['subfield']}_{obj['id']}_psf.fits\")\n",
    "    #use integration time as weight\n",
    "    int_time = np.array(param_table['DELTATIM'])\n",
    "    weights = int_time/np.sum(int_time)\n",
    "    #psf_lis to contain individual psf\n",
    "    psf_lis = fits.HDUList(); psf_lis.append(fits.PrimaryHDU())\n",
    "    #_combine to store weighted average\n",
    "    ha_combine = fits.ImageHDU(data=np.zeros((50,50)));hb_combine = fits.ImageHDU(data=np.zeros((50,50)))\n",
    "    \n",
    "    #calculate for each individual beam file:\n",
    "    for i,row in tqdm(enumerate(param_table)):\n",
    "        print(row)\n",
    "        identifier, rootname, pad, filter, pa ,focus, coord_ha, coord_hb, wavelen_ha, wavelen_hb, deltatim = row\n",
    "\n",
    "#------------------this part generate and rotate individual psf--------------------------------\n",
    "        #ha\n",
    "        filename_ha = f\"psf/individual_psf/{identifier.split('_')[2]}_ha.fits\"\n",
    "        tinytim.make_subsampled_model_psf(filename=filename_ha,\n",
    "                            psf_size = 4,\n",
    "                            filter_name = filter,\n",
    "                            focus = focus,  \n",
    "                            psf_position = coord_ha,\n",
    "                            mono = wavelen_ha,\n",
    "                            subsampling_factor = 2,\n",
    "                            exist_skip=False)\n",
    "        ha_psf = rotate_and_crop(filename_ha,-pa)\n",
    "        ha_psf.name= f'{rootname}_ha'\n",
    "        ha_combine.data += ha_psf.data * weights[i]\n",
    "        \n",
    "        #hb\n",
    "        filename_hb = f\"psf/individual_psf/{identifier.split('_')[2]}_hb.fits\"\n",
    "        tinytim.make_subsampled_model_psf(filename=filename_hb,\n",
    "                            psf_size = 4,\n",
    "                            filter_name = filter,\n",
    "                            focus = focus,  \n",
    "                            psf_position = coord_hb,\n",
    "                            mono = wavelen_hb,\n",
    "                            subsampling_factor = 2,\n",
    "                            exist_skip=False)\n",
    "        hb_psf = rotate_and_crop(filename_hb,-pa)\n",
    "        hb_psf.name = f'{rootname}_hb'\n",
    "        hb_combine.data += hb_psf.data * weights[i]\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "        #normalization and append individual psf\n",
    "        ha_psf.data = ha_psf.data/np.sum(ha_psf.data)\n",
    "        hb_psf.data = hb_psf.data/np.sum(hb_psf.data)\n",
    "        psf_lis.append(ha_psf);psf_lis.append(hb_psf)\n",
    "        os.remove(filename_ha);os.remove(filename_hb)\n",
    "\n",
    "    #save combined & individual psf\n",
    "    psf_lis.append(ha_combine);psf_lis.append(hb_combine)\n",
    "    psf_lis.writeto(f'{save_path}_psf.fits',overwrite=True)\n",
    "    ha_combine.writeto(f'{save_path_combined}_ha.fits',overwrite=True)\n",
    "    hb_combine.writeto(f'{save_path_combined}_hb.fits',overwrite=True)\n",
    "\n",
    "    #clear output and return\n",
    "    clear_output(wait=True)\n",
    "    return f\"{obj['subfield']}_{obj['id']} processed\"\n",
    "\n",
    "    #except Exception as e:\n",
    "    #    return f\"!error {obj['subfield']}_{obj['id']}:{e}\"\n",
    "\n",
    "\n",
    "\n",
    "from concurrent.futures     import ThreadPoolExecutor, as_completed\n",
    "def cat_process_gen_psf_from_input_param(obj_lis,max_threads=1):\n",
    "    #make directories:\n",
    "    os.makedirs('psf/individual_psf',exist_ok=True)\n",
    "    os.makedirs('psf/combined_psf',exist_ok=True)  \n",
    "    results = []\n",
    "    if max_threads>1:\n",
    "        with ThreadPoolExecutor(max_threads) as executor:\n",
    "            futures = {executor.submit(gen_psf,obj):obj for obj in obj_lis}\n",
    "            for future in tqdm(as_completed(futures), total=len(obj_lis), desc=\"Processing\"):\n",
    "                results.append(future.result())\n",
    "        return results\n",
    "    else:\n",
    "        for obj in tqdm(obj_lis):\n",
    "            results.append(gen_psf(obj))\n",
    "        return results\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    obj_lis = Table.read('obj_lis_selected.fits')\n",
    "    results = cat_process_gen_psf_from_input_param(obj_lis,max_threads=1)\n",
    "    number = 0\n",
    "    for result in results:\n",
    "        if 'error' in result:\n",
    "            number +=1\n",
    "            print(result)\n",
    "    print('total number of obj processed:',len(results))\n",
    "    print('number of failed obj',number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
