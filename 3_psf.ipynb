{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point spread function using Tiny Tim/Galfit\n",
    "\n",
    "Calling tinytim to generate psf is realized by .py script modified from Bryan R. Gillis, the following infomation is required:\n",
    "\n",
    "0. sensor name\n",
    "\n",
    "1. detector position\n",
    "\n",
    "2. wavelengths of the emission lines (redshift)\n",
    "\n",
    "3. subsampled factor\n",
    "\n",
    "4. (optional) filter used and focus info\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import modules and defing a few useful little functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336\n"
     ]
    }
   ],
   "source": [
    "import psf.make_psf as tinytim\n",
    "import  numpy               as     np\n",
    "from    astropy.table       import Table\n",
    "from    astropy.io          import fits\n",
    "from    scipy.ndimage       import rotate\n",
    "#from    astropy.cosmology   import Planck18\n",
    "#import  astropy.units       as     u\n",
    "import  matplotlib.pyplot   as     plt\n",
    "import  matplotlib.colors   as     colors  \n",
    "from    matplotlib          import use\n",
    "from    tqdm.auto                import tqdm\n",
    "from concurrent.futures     import ThreadPoolExecutor, as_completed\n",
    "import sys, os\n",
    "from IPython.display import clear_output\n",
    "from scripts.tools   import *\n",
    "import re\n",
    "\n",
    "obj_lis = Table.read('obj_lis_selected.fits')\n",
    "print(len(obj_lis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  gen imput parameters for tinytim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99370d97bcb24c3e83b4a1d47fc9227b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/336 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of obj processed: 336\n",
      "number of failed obj 0\n"
     ]
    }
   ],
   "source": [
    "def gen_psf_input_param(obj):\n",
    "    try:\n",
    "        col_names = ['identifier', 'rootname',  'pad', 'filter', 'pa' ,'focus', 'coord_ha', 'coord_hb', 'wavelen_ha', 'wavelen_hb','DELTATIM']\n",
    "        beam_path = f\"data_products/{file_name(obj,'beams')}\"\n",
    "\n",
    "                #chekc` if the file already exists`\n",
    "        if os.path.exists(f\"psf/obj_param/{obj['subfield']}_{obj['id']}_psf.fits\"):\n",
    "            return f\"{obj['subfield']}_{obj['id']} already exists\"\n",
    "\n",
    "        with fits.open(beam_path) as hdu:\n",
    "            rows = []\n",
    "            for image in hdu:\n",
    "                if image.name == 'SCI':\n",
    "                    #rootname, pad, shape filter\n",
    "                    rootname = image.header['ROOTNAME']\n",
    "                    identifier = f\"{obj['subfield']}_{obj['id']}_{image.header['ROOTNAME']}\"\n",
    "                    pad        = image.header.get('PAD',0)\n",
    "                    shape      = np.array(image.data.shape)\n",
    "                    filter     = image.header['filter']\n",
    "\n",
    "                    #thumbnail rel. pos\n",
    "                    x0_crop = image.header['ORIGINX']\n",
    "                    y0_crop = image.header['ORIGINY']\n",
    "\n",
    "                    # this part need actual pixel coord for each Ha and Hb wavelen\n",
    "                    coord_ha =  np.array((x0_crop,y0_crop)) - pad  #shape/2 +\n",
    "                    coord_hb =  np.array((x0_crop,y0_crop)) - pad  #shape/2 \n",
    "\n",
    "                    # this still needs focus information\n",
    "                    focus = 0\n",
    "                    #positon angle\n",
    "                    pa = image.header['ORIENTAT']\n",
    "\n",
    "                    #generate spectrum for halpha and hb\n",
    "                    wavelen_ha = 656.28*(1+obj['z_MAP'])\n",
    "                    wavelen_hb = 486.13*(1+obj['z_MAP'])\n",
    "\n",
    "                    #integrationtime\n",
    "                    deltatim = image.header['DELTATIM']\n",
    "\n",
    "                    rows.append((identifier, rootname,  pad, filter, pa ,focus, coord_ha, coord_hb, wavelen_ha, wavelen_hb,deltatim))\n",
    "            \n",
    "            #save tiny tim & drizzle input param table\n",
    "            Table(rows=rows,names=col_names).write(f\"psf/obj_param/{obj['subfield']}_{obj['id']}_psf.fits\",overwrite=True)\n",
    "            return f\"{obj['subfield']}_{obj['id']} processed\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {obj['subfield']}_{obj['id']}: {e}\")\n",
    "        return f\"!error {obj['subfield']}_{obj['id']}: {e}\"\n",
    "\n",
    "\n",
    "from concurrent.futures     import ThreadPoolExecutor, as_completed\n",
    "def cat_process_gen_psf_input_param(obj_lis,max_threads=1):\n",
    "    #make directories:\n",
    "    os.makedirs('psf/obj_param/',exist_ok=True)\n",
    "    results = []\n",
    "    if max_threads>1:\n",
    "        with ThreadPoolExecutor(max_threads) as executor:                                                       \n",
    "            futures = {executor.submit(gen_psf_input_param,obj):obj for obj in obj_lis}\n",
    "            for future in tqdm(as_completed(futures), total=len(obj_lis), desc=\"Processing\"):\n",
    "                results.append(future.result())\n",
    "        return results\n",
    "    else:\n",
    "        for obj in tqdm(obj_lis[obj_lis['manual_select']=='keep']):\n",
    "            results.append(gen_psf_input_param(obj))\n",
    "        return results\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    obj_lis = Table.read('obj_lis_selected.fits')\n",
    "    results = cat_process_gen_psf_input_param(obj_lis,max_threads=15)\n",
    "    number = 0\n",
    "    for result in results:\n",
    "        if 'error' in result:\n",
    "            number +=1\n",
    "            print(result)\n",
    "    print('total number of obj processed:',len(results))\n",
    "    print('number of failed obj',number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0269fccedbb04d11909b92be2ac4a376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of exposure per obj: 45.08041958041958\n",
      "number of pas per exposure: 4.5524475524475525\n",
      "number of unique pas: 65\n",
      "number of exposures per pa:\n",
      "average number of exposures in 102 grism 229.74468085106383\n",
      "average number of exposures in 141 grism 104.75\n"
     ]
    }
   ],
   "source": [
    "pas = np.array([])\n",
    "filters = np.array([])\n",
    "num_pas_per_exposure = np.array([])\n",
    "num_exposure_per_obj = np.array([])\n",
    "\n",
    "keep = obj_lis['manual_select']=='keep'\n",
    "for obj in tqdm(obj_lis[keep]): \n",
    "    try:\n",
    "        table  = Table.read(f\"psf/obj_param/{obj['subfield']}_{obj['id']}_psf.fits\")\n",
    "        pas    = np.append(pas,table['pa'])\n",
    "        filters = np.append(filters,table['filter'])\n",
    "        num_pas_per_exposure = np.append(num_pas_per_exposure,len(np.unique(np.round(table['pa'],decimals=1))))\n",
    "        num_exposure_per_obj = np.append(num_exposure_per_obj,len(table))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {obj['subfield']}_{obj['id']}: {e}\")\n",
    "        continue\n",
    "\n",
    "print('number of exposure per obj:',np.mean(num_exposure_per_obj))\n",
    "print('number of pas per exposure:',np.mean(num_pas_per_exposure))\n",
    "print('number of unique pas:',len(np.unique(np.round(pas,1))))\n",
    "print('number of exposures per pa:')\n",
    "g102 = filters==b'G102'\n",
    "g141 = filters==b'G141'\n",
    "\n",
    "\n",
    "print('average number of exposures in 102 grism',np.mean(np.unique(np.round(pas,1)[g102],return_counts=True)[1]))\n",
    "print('average number of exposures in 141 grism',np.mean(np.unique(np.round(pas,1)[g141],return_counts=True)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate psf by calling tinytim + rotation and crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! GN7-22746 failed, error:Format could not be identified based on the file name or contents, please provide a 'format' argument.\n",
      "The available formats are:\n",
      "           Format           Read Write Auto-identify Deprecated\n",
      "--------------------------- ---- ----- ------------- ----------\n",
      "                      ascii  Yes   Yes            No           \n",
      "               ascii.aastex  Yes   Yes            No           \n",
      "                ascii.basic  Yes   Yes            No           \n",
      "                  ascii.cds  Yes    No            No           \n",
      "     ascii.commented_header  Yes   Yes            No           \n",
      "                  ascii.csv  Yes   Yes           Yes           \n",
      "              ascii.daophot  Yes    No            No           \n",
      "                 ascii.ecsv  Yes   Yes           Yes           \n",
      "           ascii.fast_basic  Yes   Yes            No           \n",
      "ascii.fast_commented_header  Yes   Yes            No           \n",
      "             ascii.fast_csv  Yes   Yes            No           \n",
      "       ascii.fast_no_header  Yes   Yes            No           \n",
      "             ascii.fast_rdb  Yes   Yes            No           \n",
      "             ascii.fast_tab  Yes   Yes            No           \n",
      "          ascii.fixed_width  Yes   Yes            No           \n",
      "ascii.fixed_width_no_header  Yes   Yes            No           \n",
      " ascii.fixed_width_two_line  Yes   Yes            No           \n",
      "                 ascii.html  Yes   Yes           Yes           \n",
      "                 ascii.ipac  Yes   Yes            No           \n",
      "                ascii.latex  Yes   Yes           Yes           \n",
      "                  ascii.mrt  Yes   Yes            No           \n",
      "            ascii.no_header  Yes   Yes            No           \n",
      "                  ascii.qdp  Yes   Yes           Yes           \n",
      "                  ascii.rdb  Yes   Yes           Yes           \n",
      "                  ascii.rst  Yes   Yes            No           \n",
      "           ascii.sextractor  Yes    No            No           \n",
      "                  ascii.tab  Yes   Yes            No           \n",
      "                       asdf  Yes   Yes           Yes           \n",
      "                       fits  Yes   Yes           Yes           \n",
      "                       hdf5  Yes   Yes           Yes           \n",
      "                 pandas.csv  Yes   Yes            No           \n",
      "                 pandas.fwf  Yes    No            No           \n",
      "                pandas.html  Yes   Yes            No           \n",
      "                pandas.json  Yes   Yes            No           \n",
      "                    parquet  Yes   Yes           Yes           \n",
      "                    votable  Yes   Yes           Yes           \n",
      "                     aastex  Yes   Yes            No        Yes\n",
      "                        cds  Yes    No            No        Yes\n",
      "                        csv  Yes   Yes            No        Yes\n",
      "                    daophot  Yes    No            No        Yes\n",
      "                       html  Yes   Yes            No        Yes\n",
      "                       ipac  Yes   Yes            No        Yes\n",
      "                      latex  Yes   Yes            No        Yes\n",
      "                        mrt  Yes   Yes            No        Yes\n",
      "                        rdb  Yes   Yes            No        Yes\n",
      "total number of obj processed: 336\n",
      "number of failed obj 1\n"
     ]
    }
   ],
   "source": [
    "def rotate_and_crop(image_path,rotating_angle=0,crop_size=30):\n",
    "    with fits.open(image_path) as hdu:\n",
    "\n",
    "        rot = rotate(hdu[0].data, \n",
    "                    angle=rotating_angle, \n",
    "                    reshape=False, \n",
    "                    mode='constant')\n",
    "        \n",
    "        center_x, center_y = np.array(hdu[0].data.shape) // 2\n",
    "        half_crop = crop_size // 2\n",
    "        cropped_data = rot[\n",
    "            center_y - half_crop : center_y + half_crop,\n",
    "            center_x - half_crop : center_x + half_crop]\n",
    "        image = fits.ImageHDU(data=cropped_data, header=hdu[0].header)\n",
    "    return image\n",
    "\n",
    "#using tinytim and rotate to generate single psf\n",
    "def gen_psf(obj,exist_skip=False):\n",
    "    try:\n",
    "        #path to save individual psf and combined psf\n",
    "        save_path          = f\"psf/individual_psf/{obj['subfield']}_{obj['id']}\"\n",
    "        save_path_combined = f\"psf/combined_psf/{obj['subfield']}_{obj['id']}\"\n",
    "\n",
    "        #check if files already exist\n",
    "        if os.path.exists(f\"{save_path}_psf.fits\") and os.path.exists(f\"{save_path_combined}_ha.fits\") and os.path.exists(f\"{save_path_combined}_hb.fits\"):\n",
    "            clear_output(wait=True)\n",
    "            return f\"{obj['subfield']}_{obj['id']} already exists\"\n",
    "        else:\n",
    "            print(f\"Processing {obj['subfield']}_{obj['id']}\")\n",
    "        #load parameter table\n",
    "        param_table = Table.read(f\"psf/obj_param/{obj['subfield']}_{obj['id']}_psf.fits\")\n",
    "        #use integration time as weight\n",
    "        int_time = np.array(param_table['DELTATIM'])\n",
    "        weights = int_time/np.sum(int_time)\n",
    "        #psf_lis to contain individual psf\n",
    "        psf_lis = fits.HDUList(); psf_lis.append(fits.PrimaryHDU())\n",
    "        #_combine to store weighted average\n",
    "        ha_combine = fits.ImageHDU(data=np.zeros((30,30)));hb_combine = fits.ImageHDU(data=np.zeros((30,30)))\n",
    "        \n",
    "        #calculate for each individual beam file:\n",
    "        for i,row in tqdm(enumerate(param_table)):\n",
    "            print(row)\n",
    "            identifier, rootname, pad, filter, pa ,focus, coord_ha, coord_hb, wavelen_ha, wavelen_hb, deltatim = row\n",
    "\n",
    "    #------------------this part generate and rotate individual psf--------------------------------\n",
    "            #ha\n",
    "            filename_ha = f\"psf/individual_psf/{identifier.split('_')[2]}_ha.fits\"\n",
    "            tinytim.make_subsampled_model_psf(filename=filename_ha,\n",
    "                                psf_size = 4,\n",
    "                                filter_name = filter,\n",
    "                                focus = focus,  \n",
    "                                psf_position = coord_ha,\n",
    "                                mono = wavelen_ha,\n",
    "                                subsampling_factor = 2,\n",
    "                                exist_skip=True)\n",
    "            ha_psf = rotate_and_crop(filename_ha,-pa)\n",
    "            ha_psf.name= f'{rootname}_ha'\n",
    "            ha_combine.data += ha_psf.data * weights[i]\n",
    "            \n",
    "            #hb\n",
    "            filename_hb = f\"psf/individual_psf/{identifier.split('_')[2]}_hb.fits\"\n",
    "            tinytim.make_subsampled_model_psf(filename=filename_hb,\n",
    "                                psf_size = 4,\n",
    "                                filter_name = filter,\n",
    "                                focus = focus,  \n",
    "                                psf_position = coord_hb,\n",
    "                                mono = wavelen_hb,\n",
    "                                subsampling_factor = 2,\n",
    "                                exist_skip=True)\n",
    "            hb_psf = rotate_and_crop(filename_hb,-pa)\n",
    "            hb_psf.name = f'{rootname}_hb'\n",
    "            hb_combine.data += hb_psf.data * weights[i]\n",
    "    #----------------------------------------------------------------------------------------------\n",
    "\n",
    "            #normalization and append individual psf\n",
    "            ha_psf.data = ha_psf.data/np.sum(ha_psf.data)\n",
    "            hb_psf.data = hb_psf.data/np.sum(hb_psf.data)\n",
    "            psf_lis.append(ha_psf);psf_lis.append(hb_psf)\n",
    "            os.remove(filename_ha);os.remove(filename_hb)\n",
    "\n",
    "        #save combined & individual psf\n",
    "        psf_lis.append(ha_combine);psf_lis.append(hb_combine)\n",
    "        psf_lis.writeto(f'{save_path}_psf.fits',overwrite=True)\n",
    "        ha_combine.writeto(f'{save_path_combined}_ha.fits',overwrite=True)\n",
    "        hb_combine.writeto(f'{save_path_combined}_hb.fits',overwrite=True)\n",
    "\n",
    "        #clear output and return\n",
    "        clear_output(wait=True)\n",
    "        return f\"{obj['subfield']}_{obj['id']} processed\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {obj['subfield']}_{obj['id']}: {e}\")\n",
    "        return f\"! {obj['subfield']}-{obj['ID']} failed, error:{e}\"\n",
    "\n",
    "\n",
    "\n",
    "from concurrent.futures     import ThreadPoolExecutor, as_completed\n",
    "def cat_process_gen_psf_from_input_param(obj_lis,max_threads=1):\n",
    "    #make directories:\n",
    "    os.makedirs('psf/individual_psf',exist_ok=True)\n",
    "    os.makedirs('psf/combined_psf',exist_ok=True)  \n",
    "    results = []\n",
    "    if max_threads>1:\n",
    "        with ThreadPoolExecutor(max_threads) as executor:\n",
    "            futures = {executor.submit(gen_psf,obj):obj for obj in obj_lis}\n",
    "            for future in tqdm(as_completed(futures), total=len(obj_lis), desc=\"Processing\"):\n",
    "                results.append(future.result())\n",
    "        return results\n",
    "    else:\n",
    "        for obj in tqdm(obj_lis):\n",
    "            results.append(gen_psf(obj))\n",
    "        return results\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    obj_lis = Table.read('obj_lis_selected.fits')\n",
    "    #obj_lis = Table.read('failed_objects.fits')\n",
    "    results = cat_process_gen_psf_from_input_param(obj_lis,max_threads=15)\n",
    "    number = 0\n",
    "    error_table = Table(names=['subfield', 'ID', 'error'], dtype=['str', 'str', 'str'])\n",
    "    for result in results:\n",
    "        if 'error' in result:\n",
    "            number += 1\n",
    "            # 用正则提取\n",
    "            m = re.match(r\"! ([^-]+)-([^\\s]+) failed, error:(.*)\", result)\n",
    "            if m:\n",
    "                subfield, id_, error = m.group(1), m.group(2), m.group(3)\n",
    "            else:\n",
    "                subfield, id_, error = '', '', result\n",
    "            error_table.add_row([subfield, id_, error])\n",
    "            print(result)\n",
    "    error_table.write('failed_objects.fits', overwrite=True)\n",
    "\n",
    "    print('total number of obj processed:',len(results))\n",
    "    print('number of failed obj',number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><i>Table length=1</i>\n",
       "<table id=\"table4835935952\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>subfield</th><th>ID</th><th>error</th></tr></thead>\n",
       "<thead><tr><th>bytes3</th><th>bytes5</th><th>bytes102</th></tr></thead>\n",
       "<tr><td>GN7</td><td>22746</td><td>Format could not be identified based on the file name or contents, please provide a &apos;format&apos; argument.</td></tr>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<Table length=1>\n",
       "subfield ...\n",
       " bytes3  ...\n",
       "-------- ...\n",
       "     GN7 ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_table = Table.read('failed_objects.fits')\n",
    "error_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
